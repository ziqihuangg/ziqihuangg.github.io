---
permalink: /
title: "Ziqi Huang"
excerpt: "Ziqi Huang"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

Ziqi Huang is a Ph.D. student at [MMLab@NTU](https://www.mmlab-ntu.com), [Nanyang Technological University](https://www.ntu.edu.sg), supervised by [Prof. Ziwei Liu](https://liuziwei7.github.io). She is broadly interested in computer vision and deep learning. She currently works on generative models, visual generation and manipulation.

Ziqi obtained her Bachelor's degree (2022) from [NTU](https://www.ntu.edu.sg), [School of Electrical and Electronic Engineering](https://www.ntu.edu.sg/eee). She is awarded the [Google PhD Fellowship 2023](https://research.google/outreach/phd-fellowship/recipients/), and is a recipient of the [2025 Apple Scholars in AI/ML PhD Fellowship](https://machinelearning.apple.com/updates/apple-scholars-aiml-2025). 
<!-- Previously, she had internships at [ByteDance AI Lab](https://ailab.bytedance.com) and [I2R A*STAR](https://www.a-star.edu.sg/i2r).  -->

[Google Scholar](https://scholar.google.com/citations?user=Y3h_pzMAAAAJ&hl=en) / [GitHub](https://github.com/ziqihuangg) / [X](https://x.com/ziqi_huang_) / [Email](mailto:ZIQI002@e.ntu.edu.sg)


News
-----
<style>
.news-scroll {
  max-height: 14.4em;     /* 10 lines × 1.6em line height */
  line-height: 1.6em;
  overflow-y: auto;
  padding-right: 10px;
  margin-bottom: 1em;
  border-left: 4px solid #eee;
  padding-left: 10px;
}
/* Optional scrollbar style for WebKit (Chrome, Safari) */
.news-scroll::-webkit-scrollbar {
  width: 6px;
}
.news-scroll::-webkit-scrollbar-thumb {
  background: rgba(0, 0, 0, 0.2);
  border-radius: 4px;
}
.news-scroll::-webkit-scrollbar-track {
  background: transparent;
}
</style>
<div class="news-scroll">
∙ [2025-06] We are organizing CVPR 2025 Tutorial <a href="https://world-model-tutorial.github.io">From Video Generation to World Model</a><br />
∙ [2025-05] One paper (<a href="https://gordonchen19.github.io/STENCIL-Project/">Stencil</a>) accepted to <a href="https://2025.ieeeicip.org">ICIP 2025</a> as <span style="color:red; font-weight:bold">Spotlight</span><br />
∙ [2025-05] One paper (<a href="https://arxiv.org/abs/2412.09645">Evaluation Agent</a>) accepted to <a href="https://2025.aclweb.org">ACL 2025</a> (Main Conference) as <span style="color:red; font-weight:bold">Oral</span> and <span style="color:red; font-weight:bold">SAC Highlights Award</span> (47 out of 8350)<br />
∙ [2025-03] Awarded <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025">2025 Apple Scholars in AI/ML PhD Fellowship</a><br />
∙ [2025-01] Invited talk at <a href="https://www.fudan.edu.cn">Fudan University</a> <br />
∙ [2024-11] Guest lecture at <a href="https://umich.edu">University of Michigan</a><br />
∙ [2024-10] One paper (<a href="https://arxiv.org/abs/2309.15103">LaVie</a>) accepted to <a href="https://link.springer.com/journal/11263">IJCV</a><br />
∙ [2024-09] Recognized as <a href="https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers">ECCV 2024 Outstanding Reviewer</a><br />
∙ [2024-08] One paper (<a href="https://arxiv.org/abs/2303.13495">ReVersion</a>) accepted to <a href="https://asia.siggraph.org/2024/">SIGGRAPH Asia 2024</a><br />
∙ [2024-07] Invited talk at <a href="https://opendatalab.com">OpenDataLab</a> <br />
∙ [2024-07] One paper (<a href="https://arxiv.org/abs/2312.07537">FreeInit</a>) accepted to <a href="https://eccv.ecva.net">ECCV 2024</a><br />
∙ [2024-06] Invited talk at CVPR 2024 Workshop on <a href="https://sites.google.com/view/loveucvpr24/home">Long Video Understanding</a> <br />
∙ [2024-04] We are organizing CVPR 2024 Workshop <a href="https://sites.google.com/view/loveucvpr24/home">LOVEU</a><br />
∙ [2024-02] Two papers (<a href="https://arxiv.org/abs/2311.17982">VBench</a> & <a href="https://arxiv.org/abs/2309.11497">FreeU</a>) accepted to <a href="https://cvpr.thecvf.com">CVPR 2024</a> (<span style="color:red; font-weight:bold">Highlight</span> and <span style="color:red; font-weight:bold">Oral</span>)<br />
∙ [2023-12] One paper (<a href="https://ziqihuangg.github.io/papers/2023TPAMI-TalktoEdit3D.pdf">Talk-to-Edit</a>) accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a><br />
∙ [2023-12] We are organizing CVPR 2024 Workshop <a href="https://sites.google.com/view/wicv-cvpr-2024/">Women in Computer Vision (WiCV)</a><br />
∙ [2023-11] Awarded <a href="https://research.google/outreach/phd-fellowship/recipients/">Google PhD Fellowship 2023</a><br />
∙ [2023-11] Invited talk at <a href="https://research.adobe.com">Adobe Research</a> <br />
∙ [2023-06] Invited talk at <a href="https://zhidx.com/p/380937.html">Zhi Dongxi</a> <br />
∙ [2023-10] Awarded Outstanding Reviewer for <a href="https://cveu.github.io">ICCV 2023 Workshop CVEU</a><br />
∙ [2023-02] One paper (<a href="https://arxiv.org/abs/2304.10530">Collaborative Diffusion</a>) accepted to <a href="https://cvpr.thecvf.com">CVPR 2023</a><br />
∙ [2022-12] We are organizing CVPR 2023 Workshop <a href="https://sites.google.com/view/wicvcvpr2023">Women in Computer Vision (WiCV)</a><br />
∙ [2022-09] Awarded <a href="https://undergraduateawards.com/winners/regional-winners-2022">The Global Undergraduate Award</a> Asia Regional Winner<br />
∙ [2022-08] We are organizing ECCV 2022 Workshop <a href="https://sense-human.github.io">SenseHuman</a><br />
∙ [2022-08] Start my journey at <a href="https://www.mmlab-ntu.com">MMLab@NTU</a> as a Ph.D. student<br />
∙ [2022-06] Awarded <a href="https://www.ntu.edu.sg/eee/about-us/student-awards">Lee Kuan Yew Gold Medal</a><br />
∙ [2021-07] One paper (<a href="https://arxiv.org/abs/2109.04425">Talk-to-Edit</a>) accepted to <a href="http://iccv2021.thecvf.com">ICCV 2021</a><br />
∙ [2021-05] One paper (<a href="https://ziqihuangg.github.io/papers/2021ICIP-AnalogicalVQA.pdf">AnalogicalVQA</a>) accepted to <a href="https://www.2021.ieeeicip.org">ICIP 2021</a>
</div>

Education
-----
<img style="float: left; margin:5px 10px" src="../images/NTU_logo.png" width="160" height="140">
### Nanyang Technological University
<p style="line-height:1.0">
<font size="2">
Aug 2022 - Present<br />
Ph.D. student at MMLab@NTU<br />
</font>
</p>
-----
<img style="float: left; margin:5px 10px" src="../images/NTU_logo.png" width="160" height="140">
### Nanyang Technological University
<p style="line-height:1.0">
<font size="2">
Aug 2018 - May 2022<br />
B.E. in Information Engineering & Media<br />
<strong>CGPA: 5.00 / 5.00 (ranking: 1)</strong>
</font>
</p>


Publications & Preprints
-----
<style>
.teaser-img {
  float: left;
  margin: 5px 10px;
  width: 160px;
  height: 110px;
  object-fit: contain;
  object-position: 50% 50%;
  background-color: #fff;
  display: block;
}
</style>
<style>
.teaser-img-tall {
  float: left;
  margin: 5px 10px;
  width: 160px;
  height: 160px;
  object-fit: contain;
  object-position: 50% 50%;
  background-color: #fff;
  display: block;
}
</style>
<img class="teaser-img" src="../images/paper_teasers/cinescale.png">
### CineScale: Free Lunch in High-Resolution Cinematic Visual Generation
<p style="line-height:1.0">
<font size="2">
Haonan Qiu, Ning Yu†, <strong>Ziqi Huang</strong>, Paul Debevec, Ziwei Liu†<br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2508.15774">Paper</a> |
<a href="https://github.com/Eyeline-Labs/CineScale">Code</a> |
<a href="https://eyeline-labs.github.io/CineScale/">Project Page</a> |
<a href="https://www.youtube.com/watch?v=bDYmXpNctc4">Video</a>

<br />
</font>
</p>
-----<img class="teaser-img" src="../images/paper_teasers/mavis.png">
### MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling
<p style="line-height:1.0">
<font size="2">
Qian Wang, <strong>Ziqi Huang</strong>, Ruoxi Jia, Paul Debevec, Ning Yu<br />
arXiv Preprint, 2025<br />
<a href="https://www.arxiv.org/abs/2508.08487">Paper</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/cut2next.jpg">
### Cut2Next: Generating Next Shot via In-Context Tuning
<p style="line-height:1.0">
<font size="2">
Jingwen He, Hongbo Liu, Jiajun Li, <strong>Ziqi Huang</strong>, Yu Qiao, Wanli Ouyang, Ziwei Liu<br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2508.08244">Paper</a> | 
<a href="https://vchitect.github.io/Cut2Next-project/">Project Page</a> |
<a href="https://www.youtube.com/watch?v=4-olVWzeXGY">Video</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/stencil_thumbnail.jpg">
### Stencil: Subject-Driven Generation with Context Guidance
<p style="line-height:1.0">
<font size="2">
Gordon Chen, <strong>Ziqi Huang</strong>, Cheston Tan, Ziwei Liu<br />
IEEE International Conference on Image Processing (<strong>ICIP</strong>), 2025, <span style="color:red; font-weight:bold">(Spotlight)</span><br />
<a href="https://gordonchen19.github.io/STENCIL.github.io/static/pdfs/chen.pdf">Paper</a> | 
<a href="https://github.com/GordonChen19/STENCIL">Code</a> |
<a href="https://gordonchen19.github.io/STENCIL.github.io/">Project Page</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/shotbench.jpg">
### ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models
<p style="line-height:1.0">
<font size="2">
Hongbo Liu*, Jingwen He*, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, <strong>Ziqi Huang</strong>, Yinan He, Yangguang Li, Weichao Chen, Yu Qiao, Wanli Ouyang, Shengjie Zhao†, Ziwei Liu†<br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2506.21356">Paper</a> | 
<a href="https://github.com/Vchitect/ShotBench">Code</a> |
<a href="https://vchitect.github.io/ShotBench-project/">Project Page</a> |
<a href="https://www.youtube.com/watch?v=MJBJlJEsPFM&t=6s">Video</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/evaluation_agent.gif">
### Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models
<p style="line-height:1.0">
<font size="2">
Fan Zhang*,  Shulin Tian*, <strong>Ziqi Huang‡*</strong>, Yu Qiao†, Ziwei Liu† (‡: project lead)<br />
Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2025 (Main Conference), <span style="color:red; font-weight:bold">(Oral)</span> and <span style="color:red; font-weight:bold">SAC Highlights Award</span> (47 out of 8350)<br />
<a href="https://arxiv.org/abs/2412.09645">Paper</a> | 
<a href="https://github.com/Vchitect/Evaluation-Agent">Code</a> |
<a href="https://vchitect.github.io/Evaluation-Agent-project/">Project Page</a> |
<a href="https://www.youtube.com/watch?v=RyAS23dsgP8">Video</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/vbench_2.0_cropped.jpg">
### VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness
<p style="line-height:1.0">
<font size="2">
Dian Zheng*, <strong>Ziqi Huang*</strong>, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng†, Yu Qiao†, Ziwei Liu† <br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2503.21755">Paper</a> | 
<a href="https://github.com/Vchitect/VBench">Code</a> |
<a href="https://vchitect.github.io/VBench-2.0-project/">Project Page</a> |
<a href="https://www.youtube.com/watch?v=kJrzKy9tgAc">Video</a> |
<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">Leaderboard</a> |
<a href="https://huggingface.co/Vchitect">Dataset</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/repvideo_compressed.gif">
### RepVideo: Rethinking Cross-Layer Representation for Video Generation
<p style="line-height:1.0">
<font size="2">
Chenyang Si*, Weichen Fan*, Zhengyao Lv, <strong>Ziqi Huang</strong>, Yu Qiao, Ziwei Liu <br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2501.08994">Paper</a> | 
<a href="https://github.com/Vchitect/RepVideo">Code</a> |
<a href="https://vchitect.github.io/RepVid-Webpage/">Project Page</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/vchitect-2.0_video.gif">
### Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models
<p style="line-height:1.0">
<font size="2">
Weichen Fan*, Chenyang Si*, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, <strong>Ziqi Huang</strong>, Ziyue Dong, Jingwen He, Dongwei Pan, Yi Wang, Yuming Jiang, Yaohui Wang, Peng Gao, Xinyuan Chen, Hengjie Li, Dahua Lin†, Yu Qiao†, Ziwei Liu† <br />
arXiv Preprint, 2025<br />
<a href="https://arxiv.org/abs/2501.08453">Paper</a> | 
<a href="https://github.com/Vchitect/Vchitect-2.0">Code</a> |
<a href="https://vchitect.intern-ai.org.cn/">Project Page</a> |
<a href="https://huggingface.co/datasets/Vchitect/Vchitect_T2V_DataVerse">Dataset</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/vbench++.jpg">
### VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models
<p style="line-height:1.0">
<font size="2">
<strong>Ziqi Huang*</strong>, Fan Zhang*, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin†, Yu Qiao†, Ziwei Liu† <br />
arXiv Preprint, 2024<br />
<a href="https://arxiv.org/abs/2411.13503">Paper</a> | 
<a href="https://github.com/Vchitect/VBench">Code</a> |
<a href="https://vchitect.github.io/VBench-project/">Project Page</a> |
<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">Leaderboard</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/reversion.jpg">
### ReVersion: Diffusion-Based Relation Inversion from Images
<p style="line-height:1.0">
<font size="2">
<strong>Ziqi Huang*</strong>, Tianxing Wu*, Yuming Jiang, Kelvin C.K. Chan, Ziwei Liu†<br />
<strong>SIGGRAPH Asia</strong> (Conference Track), 2024 <br />
<a href="https://arxiv.org/abs/2303.13495">Paper</a> | 
<a href="https://www.youtube.com/watch?v=pkal3yjyyKQ">Video</a> |
<a href="https://github.com/ziqihuangg/ReVersion">Code</a> |
<a href="https://ziqihuangg.github.io/projects/reversion.html">Project Page</a> |
<a href="https://drive.google.com/drive/folders/1FU1Ni-oDpxQCNYKo-ZLEfSGqO-j_Hw7X?usp=sharing">Dataset</a> |
<a href="https://huggingface.co/spaces/ChenyangSi/FreeU">Demo</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/lavie.gif">
### LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models
<p style="line-height:1.0">
<font size="2">
Yaohui Wang*, Xinyuan Chen*, Xin Ma*, Shangchen Zhou, <strong>Ziqi Huang</strong>, Yi Wang, Ceuyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin†, Yu Qiao†, Ziwei Liu† <br />
International Journal of Computer Vision (<strong>IJCV</strong>), 2024<br />
<a href="https://arxiv.org/abs/2309.15103">Paper</a> | 
<a href="https://vchitect.github.io/LaVie-project/">Project Page</a> |
<a href="https://github.com/Vchitect/LaVie">Code</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/freeinit_compressed.gif">
### FreeInit : Bridging Initialization Gap in Video Diffusion Models
<p style="line-height:1.0">
<font size="2">
Tianxing Wu, Chenyang Si, Yuming Jiang, <strong>Ziqi Huang</strong>, Ziwei Liu† <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2024 <br /> 
<a href="https://arxiv.org/abs/2312.07537">Paper</a> | 
<a href="https://youtu.be/lS5IYbAqriI">Video</a> |
<a href="https://github.com/TianxingWu/FreeInit">Code</a> |
<a href="https://tianxingwu.github.io/pages/FreeInit/">Project Page</a> |
<a href="https://huggingface.co/spaces/TianxingWu/FreeInit">Demo</a>
<br />
</font>
</p>
-----
<img class="teaser-img-tall" src="../images/paper_teasers/vbench_cropped.jpg">
### VBench: Comprehensive Benchmark Suite for Video Generative Models
<p style="line-height:1.0">
<font size="2">
<strong>Ziqi Huang*</strong>, Yinan He*, Jiashuo Yu*, Fan Zhang*, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin†, Yu Qiao†, Ziwei Liu† <br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <span style="color:red; font-weight:bold">(Highlight)</span> <br />
<a href="https://arxiv.org/abs/2311.17982">Paper</a> | 
<a href="https://www.youtube.com/watch?v=7IhCC8Qqn8Y">Video</a> |
<a href="https://github.com/Vchitect/VBench">Code</a> |
<a href="https://vchitect.github.io/VBench-project/">Project Page</a> |
<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">Leaderboard</a> |
<a href="https://huggingface.co/datasets/Vchitect/VBench_sampled_video/tree/main">Dataset</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/freeu_teaser.jpg">
### FreeU: Free Lunch in Diffusion U-Net
<p style="line-height:1.0">
<font size="2">
Chenyang Si, <strong>Ziqi Huang</strong>, Yuming Jiang, Ziwei Liu†<br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <span style="color:red; font-weight:bold">(Oral)</span><br />
<a href="https://arxiv.org/abs/2309.11497">Paper</a> | 
<a href="https://www.youtube.com/watch?v=-CZ5uWxvX30&t=3s">Video</a> |
<a href="https://github.com/ChenyangSi/FreeU">Code</a> |
<a href="https://chenyangsi.top/FreeU/">Project Page</a> |
<a href="https://huggingface.co/spaces/Ziqi/ReVersion">Demo</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/handbook_logo_cropped.png">
### Generative Networks
<p style="line-height:1.0">
<font size="2">
Ziwei Liu†, Shuai Yang, Yuming Jiang, <strong>Ziqi Huang</strong><br />
Handbook of Face Recognition, Third Edition, 2023<br />
<a href="https://link.springer.com/chapter/10.1007/978-3-031-43567-6_3">Digital Book</a> 
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/talk-to-edit-pami.png">
### Talk-to-Edit: Fine-Grained 2D and 3D Facial Editing via Dialog
<p style="line-height:1.0">
<font size="2">
Yuming Jiang, <strong>Ziqi Huang</strong>, Tianxing Wu, Xingang Pan, Chen Change Loy, Ziwei Liu†<br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023<br />
<a href="https://ziqihuangg.github.io/papers/2023TPAMI-TalktoEdit3D.pdf">PDF</a> | 
<a href="https://ieeexplore.ieee.org/document/10374263/">Paper</a> | 
<a href="https://www.mmlab-ntu.com/project/talkedit/index.html">Project Page</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/collaborative_diffusion.jpg">
### Collaborative Diffusion for Multi-Modal Face Generation and Editing
<p style="line-height:1.0">
<font size="2">
<strong>Ziqi Huang</strong>, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu†<br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023<br />
<a href="https://arxiv.org/abs/2304.10530">Paper</a> | 
<a href="https://www.youtube.com/watch?v=inLK4c8sNhc">Video</a> |
<a href="https://github.com/ziqihuangg/Collaborative-Diffusion">Code</a> |
<a href="https://ziqihuangg.github.io/projects/collaborative-diffusion.html">Project Page</a>

<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/celeba_dialog.png">
### Talk-to-Edit: Fine-Grained Facial Editing via Dialog
<p style="line-height:1.0">
<font size="2">
Yuming Jiang*, <strong>Ziqi Huang*</strong>, Xingang Pan, Chen Change Loy, Ziwei Liu†<br />
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2021<br />
<a href="https://arxiv.org/abs/2109.04425">Paper</a> | 
<a href="https://www.youtube.com/watch?v=ZKMkQhkMXPI">Video</a> |
<a href="https://github.com/yumingj/Talk-to-Edit">Code</a> |
<a href="https://www.mmlab-ntu.com/project/talkedit/index.html">Project Page</a> |
<a href="https://github.com/ziqihuangg/CelebA-Dialog">Dataset</a>
<br />
</font>
</p>
-----
<img class="teaser-img" src="../images/paper_teasers/icip2021_thumb.png">
### A Diagnostic Study of Visual Question Answering with Analogical Reasoning
<p style="line-height:1.0">
<font size="2">
<strong>Ziqi Huang</strong>, Hongyuan Zhu†, Ying Sun, Dongkyu Choi, Cheston Tan, Joo−Hwee Lim<br />
IEEE International Conference on Image Processing (<strong>ICIP</strong>), 2021<br />
<a href="https://ziqihuangg.github.io/papers/2021ICIP-AnalogicalVQA.pdf">PDF</a> | 
<a href="https://ieeexplore.ieee.org/document/9506539/">Paper</a> | 
<a href="https://www.youtube.com/watch?v=W1TLrhTKPKE">Video</a>
<br />
</font>
</p>




Awards
-----
∙ [2025] [ACL 2025 SAC Highlights Award (47 out of 8350)](https://2025.aclweb.org/program/awards/)<br />
∙ [2025] [2025 Apple Scholars in AI/ML PhD Fellowship](https://machinelearning.apple.com/updates/apple-scholars-aiml-2025)<br />
∙ [2024] [ECCV 2024 Outstanding Reviewer](https://eccv.ecva.net/Conferences/2024/Reviewers#all-outstanding-reviewers)<br />
∙ [2023] [Google PhD Fellowship 2023](https://research.google/outreach/phd-fellowship/recipients/)<br />
∙ [2023] Outstanding Reviewer, [CVEU Workshop, ICCV 2023](https://cveu.github.io)<br />
∙ [2022] [The Global Undergraduate Award](https://undergraduateawards.com/winners/regional-winners-2022) Regional Winner (Asia)<br />
∙ [2022] [Lee Kuan Yew Gold Medal](https://www.ntu.edu.sg/eee/about-us/student-awards) (Top 1 in Undergraduate Cohort)<br />
∙ [2020 & 2021] NTU President Research Scholar<br />
∙ [2019 & 2022] Dean’s List (School of Electrical and Electronic Engineering)<br />
∙ [2018] NTU Science and Engineering Undergraduate Scholarship<br />

<!-- Talk
-----
∙ [2024-06] Oral Presentation at CVPR 2024 on accepted paper "FreeU: Free Lunch in Diffusion U-Net" ([Slides](https://ziqihuangg.github.io/slides/2024_06_17_freeu_cvpr2024oral.pdf))<br />
∙ [2024-06] Invited Talk at CVPR 2024 Workshop on "VBench: Comprehensive Benchmark Suite for Video Generative Models" ([Slides](https://ziqihuangg.github.io/slides/2024_06_17_vbench_cvpr2024workshop.pdf))<br />
∙ [2023-10] Invited Talk at Adobe Research on "Exploring Free Lunch in Diffusion U-Net" ([Slides](https://ziqihuangg.github.io/slides/2023_10_30_freeu_adobe.pdf), [Video](https://ziqihuangg.github.io/slides/2023_10_30_freeu_adobe_video.mp4))<br />
∙ [2023-06] Invited Talk at Zhi Dongxi (智东西) on "Collaborative Diffusion and Human-Machine Collaborative AIGC" ([Slides](https://ziqihuangg.github.io/slides/2023_06_09_collaborative_diffusion_zhidongxi.pdf))<br /> -->

Professional Service
-----
### Workshop Organizer:<br />
∙ CVPR 2025 - Organizer - Tutorial: From Video Generation to World Model, [Website](https://world-model-tutorial.github.io)<br />
∙ CVPR 2024 - Organizer - LOVEU Workshop: Long-form Video Understanding Towards Multimodal AI Assistant and Copilot, [Website](https://sites.google.com/view/loveucvpr24/home)<br />
∙ CVPR 2024 - General Chair - Women in Computer Vision (WiCV) Workshop, [Website](https://sites.google.com/view/wicv-cvpr-2024/)<br />
∙ CVPR 2023 - Organizer - Women in Computer Vision (WiCV) Workshop, [Website](https://sites.google.com/view/wicvcvpr2023), [Report](https://arxiv.org/abs/2309.12768), [Video](https://youtu.be/hm1RA4ef7W8)<br />
∙ ECCV 2022 - Organizer - SenseHuman: Sensing, Understanding and Synthesizing Humans, [Website](https://sense-human.github.io), [Video](https://www.youtube.com/watch?v=B8eyHbdDh14)<br />

-----
### Talk Organizer:<br />
∙ [The AI Talks](https://theaitalks.org) - keeping up with the latest advances in AI<br />

-----
### Reviewer:<br />
∙ Conference Reviewer: CVPR, ICCV, ICLR, ICIP, ECCV, SIGGRAPH Asia, NeurIPS, ACCV<br />
∙ Journal Reviewer: IJCV, IET CV, PR<br />
∙ Workshop Reviewer: ICCV 2023 CVEU Workshop, CVPR 2023 WiCV Workshop

Teaching
-----
∙ Teaching Assistant: SC2001/CE2101/CZ2101 Algorithm Design & Analysis, NTU, 2024 Spring<br />
∙ Teaching Assistant: SC2001/CE2101/CZ2101 Algorithm Design & Analysis, NTU, 2023 Spring